{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ec36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SEC Topic Modeling (BERTopic) â€” single-file script that runs in Jupyter.\n",
    "\n",
    "How to use in a Jupyter notebook:\n",
    "1) Put this file in the same folder as your notebook (e.g., sec_bertopic_pipeline.py)\n",
    "2) In a notebook cell run:\n",
    "   %run sec_bertopic_pipeline.py\n",
    "\n",
    "Dependencies (run once):\n",
    "   !pip install bertopic[all] sentence-transformers pandas\n",
    "\n",
    "What it does:\n",
    "- Loads SEC .txt filings (EDGAR full submission format)\n",
    "- Extracts <DOCUMENT> blocks and their <TEXT> sections\n",
    "- Cleans + chunks the narrative text\n",
    "- Runs BERTopic and forces EXACTLY 10 topics (by reduce_topics)\n",
    "- Prints demo-ready topic summaries and writes CSV outputs\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import html\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "FILES = [\n",
    "    \"/Users/rithikreddynibbaragandlla/PycharmProjects/JupyterProject/0001193125-19-235916.txt\"\n",
    "]\n",
    "\n",
    "# If these doc TYPES exist in the filing, we prioritize them; otherwise we fall back to all types.\n",
    "PREFERRED_TYPES = {\"10-K\", \"10-Q\", \"8-K\", \"S-1\", \"S-3\", \"S-4\", \"424B5\", \"424B2\", \"424B3\", \"DEF 14A\"}\n",
    "\n",
    "# Chunking behavior\n",
    "MIN_CHUNK_CHARS = 450\n",
    "TARGET_CHUNK_CHARS = 1100\n",
    "MAX_CHUNK_CHARS = 1600\n",
    "\n",
    "# Topic modeling\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "TARGET_TOPICS = 10\n",
    "\n",
    "# Output files (written to current working directory)\n",
    "OUT_TOPICS_CSV = \"sec_topics_bertopic.csv\"\n",
    "OUT_DOCS_CSV = \"sec_docs_with_topics.csv\"\n",
    "\n",
    "\n",
    "BASE_STOPWORDS = {\n",
    "    \"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\n",
    "    \"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\n",
    "    \"but\",\"by\",\"can't\",\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"do\",\"does\",\"doesn't\",\n",
    "    \"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn't\",\n",
    "    \"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\n",
    "    \"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\n",
    "    \"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\n",
    "    \"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\n",
    "    \"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\n",
    "    \"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\n",
    "    \"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\n",
    "    \"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\n",
    "    \"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\n",
    "    \"we're\",\"we've\",\"were\",\"weren't\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\n",
    "    \"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"won't\",\"would\",\"wouldn't\",\n",
    "    \"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\"\n",
    "}\n",
    "\n",
    "# Finance-specific stopwords added on top\n",
    "FIN_EXTRA = {\n",
    "    \"section\",\"herein\",\"hereby\",\"thereof\",\"exhibit\",\"prospectus\",\"form\",\n",
    "    \"llc\",\"inc\",\"corporation\",\"company\",\"issuer\",\"tender\",\"exchange\",\n",
    "    \"may\",\"shall\",\"pursuant\",\"filing\",\"statement\",\"table\",\"contents\",\n",
    "    \"security\",\"securities\",\"notes\",\"debentures\",\"shares\",\"rate\",\"interest\",\n",
    "    \"benchmark\",\"maturity\",\"coupon\",\"floating\",\"fixed\",\"redemption\",\n",
    "    \"rule\",\"holders\",\"offering\",\"registrant\",\"corporate\",\"document\"\n",
    "}\n",
    "\n",
    "FIN_STOPWORDS = BASE_STOPWORDS | FIN_EXTRA\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = strip_html_tags_keep_text(text)\n",
    "    text = normalize_boilerplate(text)\n",
    "\n",
    "    # remove numbers, CUSIP codes, % signs, $ amounts etc.\n",
    "    text = re.sub(r\"\\$?\\d[\\d,\\.%]*\", \" \", text)\n",
    "\n",
    "    tokens = re.findall(r\"[A-Za-z]+\", text.lower())\n",
    "    tokens = [t for t in tokens if t not in FIN_STOPWORDS and len(t) > 3]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "@dataclass\n",
    "class SecDocBlock:\n",
    "    source_file: str\n",
    "    doc_type: str\n",
    "    filename: str\n",
    "    raw_text: str\n",
    "\n",
    "\n",
    "_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "_WS_RE = re.compile(r\"\\s+\")\n",
    "_DOC_BLOCK_RE = re.compile(r\"<DOCUMENT>(.*?)</DOCUMENT>\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "\n",
    "def _read_text(path: Path) -> str:\n",
    "    # EDGAR can be messy; ignore decode errors rather than fail\n",
    "    return path.read_text(errors=\"ignore\")\n",
    "\n",
    "\n",
    "def _extract_tag_value(text: str, tag: str) -> Optional[str]:\n",
    "    # Match <TAG>value up to end of line or next tag\n",
    "    m = re.search(rf\"<{tag}>\\s*([^\\n<]+)\", text, flags=re.IGNORECASE)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "\n",
    "def parse_edgar_submission(file_path: str) -> List[SecDocBlock]:\n",
    "    \"\"\"\n",
    "    Parse a full EDGAR submission .txt and return list of document blocks.\n",
    "    Each block tries to capture <TYPE>, <FILENAME>, <TEXT>.\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    full_text = _read_text(path)\n",
    "\n",
    "    blocks: List[SecDocBlock] = []\n",
    "    for doc in _DOC_BLOCK_RE.findall(full_text):\n",
    "        doc_type = _extract_tag_value(doc, \"TYPE\") or \"UNKNOWN\"\n",
    "        fname = _extract_tag_value(doc, \"FILENAME\") or \"\"\n",
    "        m_text = re.search(r\"<TEXT>(.*?)</TEXT>\", doc, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if not m_text:\n",
    "            continue\n",
    "        raw_text = m_text.group(1)\n",
    "        blocks.append(SecDocBlock(source_file=path.name, doc_type=doc_type, filename=fname, raw_text=raw_text))\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def strip_html_tags_keep_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML-ish tags and decode entities. Many SEC <TEXT> blocks are HTML.\n",
    "    \"\"\"\n",
    "    # Some documents embed HTML comments / scripts; drop common noisy things quickly.\n",
    "    text = re.sub(r\"(?is)<script.*?>.*?</script>\", \" \", text)\n",
    "    text = re.sub(r\"(?is)<style.*?>.*?</style>\", \" \", text)\n",
    "\n",
    "    text = _TAG_RE.sub(\" \", text)\n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\x00\", \" \")\n",
    "    text = _WS_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def looks_like_table_or_numbers(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic to skip chunks that are mostly numeric/table-like.\n",
    "    \"\"\"\n",
    "    if len(text) < 200:\n",
    "        return True\n",
    "    digits = sum(ch.isdigit() for ch in text)\n",
    "    ratio = digits / max(1, len(text))\n",
    "    # If it's heavily numeric, likely a table/fn that harms topics.\n",
    "    return ratio > 0.18\n",
    "\n",
    "\n",
    "def normalize_boilerplate(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light boilerplate cleanup.\n",
    "    We DON'T remove finance words; we just reduce repeated \"Table of Contents\" and weird page markers.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"(?i)\\btable of contents\\b\", \" \", text)\n",
    "    text = re.sub(r\"\\bS-\\d+\\b\", \" \", text)     # common page footer\n",
    "    text = re.sub(r\"\\bPage\\s+\\d+\\b\", \" \", text)\n",
    "    text = _WS_RE.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "_SENT_SPLIT_RE = re.compile(r\"(?<=[\\.\\!\\?])\\s+\")\n",
    "\n",
    "\n",
    "def chunk_text(text: str,\n",
    "               min_chars: int = MIN_CHUNK_CHARS,\n",
    "               target_chars: int = TARGET_CHUNK_CHARS,\n",
    "               max_chars: int = MAX_CHUNK_CHARS) -> List[str]:\n",
    "    \"\"\"\n",
    "    Sentence-ish chunking:\n",
    "    - Split to sentences\n",
    "    - Accumulate into ~target_chars chunks\n",
    "    - Enforce min/max lengths\n",
    "    \"\"\"\n",
    "    sentences = _SENT_SPLIT_RE.split(text)\n",
    "    chunks: List[str] = []\n",
    "    buf: List[str] = []\n",
    "    buf_len = 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal buf, buf_len\n",
    "        if not buf:\n",
    "            return\n",
    "        chunk = \" \".join(buf).strip()\n",
    "        buf = []\n",
    "        buf_len = 0\n",
    "        if len(chunk) >= min_chars:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "\n",
    "        # If one sentence is enormous, hard split it.\n",
    "        if len(s) > max_chars:\n",
    "            # flush current buffer first\n",
    "            flush()\n",
    "            for i in range(0, len(s), max_chars):\n",
    "                part = s[i:i + max_chars].strip()\n",
    "                if len(part) >= min_chars:\n",
    "                    chunks.append(part)\n",
    "            continue\n",
    "\n",
    "        if buf_len + len(s) + 1 <= target_chars:\n",
    "            buf.append(s)\n",
    "            buf_len += len(s) + 1\n",
    "        else:\n",
    "            flush()\n",
    "            buf.append(s)\n",
    "            buf_len = len(s) + 1\n",
    "\n",
    "    flush()\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_corpus(files: List[str],\n",
    "                 preferred_types: set[str] = PREFERRED_TYPES) -> Tuple[List[str], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      docs: list[str] chunks for BERTopic\n",
    "      meta: list[dict] per chunk with provenance\n",
    "    \"\"\"\n",
    "    all_blocks: List[SecDocBlock] = []\n",
    "    for fp in files:\n",
    "        all_blocks.extend(parse_edgar_submission(fp))\n",
    "\n",
    "    # If any preferred types exist, keep only those; otherwise keep everything\n",
    "    existing_types = {b.doc_type for b in all_blocks}\n",
    "    use_types = preferred_types & existing_types\n",
    "    filtered = [b for b in all_blocks if (b.doc_type in use_types)] if use_types else all_blocks\n",
    "\n",
    "    docs: List[str] = []\n",
    "    meta: List[Dict] = []\n",
    "\n",
    "    for b in filtered:\n",
    "        cleaned = preprocess_text(b.raw_text)\n",
    "        cleaned = normalize_boilerplate(cleaned)\n",
    "        if len(cleaned) < 1000:\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_text(cleaned)\n",
    "        for idx, ch in enumerate(chunks):\n",
    "            if looks_like_table_or_numbers(ch):\n",
    "                continue\n",
    "            docs.append(ch)\n",
    "            meta.append({\n",
    "                \"source_file\": b.source_file,\n",
    "                \"doc_type\": b.doc_type,\n",
    "                \"doc_filename\": b.filename,\n",
    "                \"chunk_id\": idx,\n",
    "                \"chunk_chars\": len(ch),\n",
    "            })\n",
    "\n",
    "    return docs, meta\n",
    "\n",
    "\n",
    "#Start of BERTopic\n",
    "\n",
    "def fit_bertopic_force_10_topics(docs: List[str]):\n",
    "    \"\"\"\n",
    "    Fit BERTopic and force exactly TARGET_TOPICS using updated API signature.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(docs) < 20:\n",
    "        raise ValueError(f\"Not enough text chunks: {len(docs)} detected.\")\n",
    "\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    # First run: loose clustering (min_topic_size small so topics form)\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        min_topic_size=5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Force exactly N topics\n",
    "    topic_model = topic_model.reduce_topics(docs, nr_topics=TARGET_TOPICS)\n",
    "\n",
    "    # Recompute assignments after reduction\n",
    "    topics, probs = topic_model.transform(docs)\n",
    "\n",
    "    return topic_model, topics, probs\n",
    "\n",
    "\n",
    "\n",
    "def topic_words(topic_model: BERTopic, topic_id: int, top_n: int = 10) -> List[str]:\n",
    "    pairs = topic_model.get_topic(topic_id) or []\n",
    "    return [w for (w, _) in pairs[:top_n]]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # --- Build corpus\n",
    "    docs, meta = build_corpus(FILES, PREFERRED_TYPES)\n",
    "    print(f\"Chunks produced for modeling: {len(docs)}\")\n",
    "    if len(docs) == 0:\n",
    "        raise RuntimeError(\"No usable text chunks were extracted. \"\n",
    "                           \"Try lowering MIN_CHUNK_CHARS, or check that the filings are valid EDGAR .txt files.\")\n",
    "\n",
    "    # --- Fit topic model\n",
    "    topic_model, topics, probs = fit_bertopic_force_10_topics(docs)\n",
    "\n",
    "    # --- Topic summary (exclude outlier topic -1 if it exists)\n",
    "    info = topic_model.get_topic_info()\n",
    "    info_non_outlier = info[info[\"Topic\"] != -1].copy()\n",
    "\n",
    "    # If reduce_topics worked, we should have exactly TARGET_TOPICS here (sometimes topic -1 exists in addition).\n",
    "    top_topics = info_non_outlier.head(TARGET_TOPICS)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in top_topics.iterrows():\n",
    "        tid = int(r[\"Topic\"])\n",
    "        rows.append({\n",
    "            \"topic_id\": tid,\n",
    "            \"count\": int(r[\"Count\"]),\n",
    "            \"auto_name\": r[\"Name\"],\n",
    "            \"top_words\": \", \".join(topic_words(topic_model, tid, top_n=12)),\n",
    "        })\n",
    "\n",
    "    topics_df = pd.DataFrame(rows).sort_values([\"count\"], ascending=False)\n",
    "    print(\"\\n=== Demo-ready topic list (10 topics) ===\")\n",
    "    for _, r in topics_df.iterrows():\n",
    "        print(f\"\\nTopic {r['topic_id']} | chunks={r['count']}\")\n",
    "        print(f\"Label: {r['auto_name']}\")\n",
    "        print(f\"Top words: {r['top_words']}\")\n",
    "\n",
    "\n",
    "    def format_topic_output(topic_model, n_words=20):\n",
    "        \"\"\"\n",
    "        Format BERTopic topics to look like LDA:\n",
    "        (topic_id, '0.027*\"word1\" + 0.023*\"word2\" + ...')\n",
    "        \"\"\"\n",
    "        topics = topic_model.get_topics()\n",
    "        formatted = []\n",
    "\n",
    "        for topic_id, word_scores in topics.items():\n",
    "            if topic_id == -1:  # Skip outlier topic\n",
    "                continue\n",
    "\n",
    "            # Format: 0.027*\"word\"\n",
    "            weighted_terms = [\n",
    "                f'{round(score, 3)}*\"{word}\"'\n",
    "                for word, score in word_scores[:n_words]\n",
    "            ]\n",
    "\n",
    "            formatted_str = \" + \".join(weighted_terms)\n",
    "            formatted.append((topic_id, formatted_str))\n",
    "\n",
    "        return formatted\n",
    "\n",
    "        # ---- RUN IT ----\n",
    "    formatted_topics = format_topic_output(topic_model, n_words=20)\n",
    "\n",
    "    for topic in formatted_topics:\n",
    "        print(topic)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run when executed via %run in Jupyter\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
