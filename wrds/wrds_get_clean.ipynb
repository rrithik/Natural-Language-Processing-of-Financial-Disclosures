{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9b27b-370a-448d-a3a9-63376b617d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n",
      "Mapping SEC public paths → WRDS internal paths...\n",
      "\n",
      "Mapping completed.\n",
      "\n",
      "Removing existing folder: /scratch/oregonstate/kuohsu/filings_text\n",
      "Saving CLEAN filings to: /scratch/oregonstate/kuohsu/filings_text\n",
      "\n",
      "Extracting filings from WRDS filesystem...\n",
      "\n",
      "\n",
      "Done!\n",
      "OK: 32784\n",
      "NO MATCH: 0\n",
      "MISSING INPUT FILE: 0\n",
      "FAILED: 0\n",
      "All CLEAN files saved under: /scratch/oregonstate/kuohsu/filings_text\n",
      "\n",
      "Output file count (clean only): 32784\n",
      "Removing existing archive: /scratch/oregonstate/kuohsu/filings_text.tar.gz\n",
      "Creating archive: /scratch/oregonstate/kuohsu/filings_text.tar.gz\n",
      "Archive created (343.77 MB)\n",
      "\n",
      "Creating split ZIP (~3000 MB per part if needed)...\n",
      "ZIP creation finished.\n",
      "Created 1 ZIP file(s):\n",
      " - /scratch/oregonstate/kuohsu/filings_text.zip\n",
      "\n",
      "Copying filings_text.zip to home: /home/oregonstate/kuohsu/filings_text.zip\n",
      " Staged filings_text.zip (393.08 MB) in home directory.\n",
      " Download it now (Jupyter UI download or scp from your local machine).\n",
      " IMPORTANT: AFTER downloading, delete it from home to free quota:\n",
      "   rm ~/filings_text.zip\n",
      "\n",
      "NOTE: Single ZIP file (not split). Extract directly in WinRAR/7-Zip.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT:\n",
    "# This code requires the WRDS-hosted Jupyter environment to run.\n",
    "# It will NOT work on a local machine because it relies on WRDS internal paths\n",
    "# and direct filesystem access to SEC archives.\n",
    "#\n",
    "# Link: https://wrds-jupyter.wharton.upenn.edu/\n",
    "#\n",
    "# Before you start, go to /scratch/oregonstate/ and create your folder\n",
    "# In this code, it uses kuohsu as the folder name. You can press Ctrl + F to replace it with your own folder name\n",
    "#\n",
    "# Required input:\n",
    "#   fname_list.csv  → must contain column: \"path\" OR \"fname\" (SEC public paths)\n",
    "#\n",
    "# Output (CLEAN ONLY):\n",
    "#   /scratch/oregonstate/kuohsu/filings_text/\n",
    "#   - Only readable .txt files (not raw)\n",
    "#   - Keeps primary filing doc + finance/legal exhibits (EX-10.*, EX-99.*)\n",
    "#   - Drops junk docs (GRAPHIC, EX-101.*, XML/XBRL, PDF/ZIP/EXCEL, etc.)\n",
    "#\n",
    "# Archive:\n",
    "#   /scratch/oregonstate/kuohsu/filings_text.zip\n",
    "#\n",
    "# Notes:\n",
    "# - Files in /scratch are temporary (~1 week retention)\n",
    "# - Only errors are printed (NO MATCH / MISSING / FAILED)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import html as ihtml\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import wrds\n",
    "from bs4 import BeautifulSoup\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "# HTML -> text cleanup\n",
    "def strip_html_to_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    # Decode HTML entities (&nbsp; etc.)\n",
    "    s = ihtml.unescape(s)\n",
    "\n",
    "    # Parse HTML (lxml is faster/more robust; fallback to html.parser if needed)\n",
    "    try:\n",
    "        soup = BeautifulSoup(s, \"lxml\")\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(s, \"html.parser\")\n",
    "\n",
    "    # Remove scripts/styles/etc.\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Optional: images usually add no useful text\n",
    "    for tag in soup([\"img\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Preserve structure: convert common tags into line breaks\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "\n",
    "    for tagname in [\"p\", \"div\", \"tr\", \"li\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"table\"]:\n",
    "        for t in soup.find_all(tagname):\n",
    "            t.append(\"\\n\")\n",
    "\n",
    "    # Extract text\n",
    "    text = soup.get_text(separator=\" \", strip=False)\n",
    "\n",
    "    # Normalize spaces (BUT keep line structure)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    # Re-attach percentages split onto their own line\n",
    "    text = re.sub(r\"(?m)(\\d+(?:\\.\\d+)?)\\s*\\n\\s*%\\s*\", r\"\\1% \", text)\n",
    "\n",
    "    # Collapse excessive blank lines (3+ → 2)\n",
    "    text = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", text)\n",
    "\n",
    "    # Remove trailing whitespace on each line\n",
    "    text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def drop_leading_xbrl_gunk(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Some newer filings have inline-XBRL / XML-ish junk at the beginning of <TEXT>.\n",
    "    Cut to the first real filing header if we can find it.\n",
    "    \"\"\"\n",
    "    markers = [\n",
    "        \"\\nUNITED STATES\\n\",\n",
    "        \"\\nSECURITIES AND EXCHANGE COMMISSION\\n\",\n",
    "        \"\\nFORM 8-K\\n\",\n",
    "        \"\\nFORM 10-K\\n\",\n",
    "        \"\\nFORM 10-Q\\n\",\n",
    "        \"\\nSCHEDULE 14A\\n\",\n",
    "        \"\\nDEFINITIVE PROXY STATEMENT\\n\",\n",
    "        \"\\nCURRENT REPORT\\n\",\n",
    "    ]\n",
    "    u = text.upper()\n",
    "    best = None\n",
    "    for m in markers:\n",
    "        idx = u.find(m)\n",
    "        if idx != -1:\n",
    "            best = idx if best is None else min(best, idx)\n",
    "    if best is not None and best > 0:\n",
    "        return text[best:].lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# SEC parsing helpers\n",
    "\n",
    "DOC_START = re.compile(r\"<DOCUMENT>\\s*\", re.IGNORECASE)\n",
    "DOC_END   = re.compile(r\"</DOCUMENT>\\s*\", re.IGNORECASE)\n",
    "TYPE_LINE = re.compile(r\"<TYPE>\\s*([^\\s<]+)\", re.IGNORECASE)\n",
    "\n",
    "SEC_HDR_START = re.compile(r\"<SEC-HEADER>\", re.IGNORECASE)\n",
    "SEC_HDR_END   = re.compile(r\"</SEC-HEADER>\", re.IGNORECASE)\n",
    "SUBMISSION_TYPE_LINE = re.compile(r\"CONFORMED SUBMISSION TYPE:\\s*([A-Z0-9\\-]+)\", re.IGNORECASE)\n",
    "\n",
    "# Junk doc types we never want in “readable text”\n",
    "SKIP_TYPES = {\n",
    "    \"GRAPHIC\", \"ZIP\", \"EXCEL\", \"PDF\", \"JSON\", \"XML\",\n",
    "    \"XBRL\", \"IDEA\", \"SCHEMA\", \"CAL\", \"DEF\", \"LAB\", \"PRE\"\n",
    "}\n",
    "\n",
    "def get_primary_form_from_header(raw: str) -> str:\n",
    "    m1 = SEC_HDR_START.search(raw)\n",
    "    m2 = SEC_HDR_END.search(raw)\n",
    "    if not m1 or not m2 or m2.start() <= m1.end():\n",
    "        return \"\"\n",
    "    hdr = raw[m1.end():m2.start()]\n",
    "    m = SUBMISSION_TYPE_LINE.search(hdr)\n",
    "    return (m.group(1).strip().upper() if m else \"\")\n",
    "\n",
    "def is_xbrl_aux(doc_type: str) -> bool:\n",
    "    t = (doc_type or \"\").upper().strip()\n",
    "    return t.startswith(\"EX-101.\") or t in {\"EX-101\", \"101\"}\n",
    "\n",
    "def should_keep_doc_type(doc_type: str, primary_form: str) -> bool:\n",
    "    t = (doc_type or \"\").upper().strip()\n",
    "    p = (primary_form or \"\").upper().strip()\n",
    "\n",
    "    if not t:\n",
    "        return False\n",
    "\n",
    "    # keep the primary filing document (8-K, DEFA14A, 10-K, etc.)\n",
    "    if p and t == p:\n",
    "        return True\n",
    "\n",
    "    # Keep finance/legal exhibits\n",
    "    if t.startswith(\"EX-10\"): \n",
    "        return True\n",
    "    if t.startswith(\"EX-99\"): \n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def clean_sec_filing(raw: str, keep_header: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Create a human-readable text output:\n",
    "    - Keep <SEC-HEADER> \n",
    "    - Keep primary <DOCUMENT> and key exhibits (EX-10.*, EX-99.*)\n",
    "    - Drop GRAPHIC/base64 and XBRL auxiliary docs\n",
    "    - Extract <TEXT> when present\n",
    "    - Strip HTML tags when detected\n",
    "    - Remove leading inline-XBRL “junk” in <TEXT> when found\n",
    "    \"\"\"\n",
    "    primary_form = get_primary_form_from_header(raw)\n",
    "\n",
    "    out_parts = []\n",
    "\n",
    "\n",
    "    if keep_header:\n",
    "        m1 = SEC_HDR_START.search(raw)\n",
    "        m2 = SEC_HDR_END.search(raw)\n",
    "        if m1 and m2 and m2.start() > m1.end():\n",
    "            out_parts.append(raw[m1.start():m2.end()].strip())\n",
    "            out_parts.append(\"\")\n",
    "\n",
    "    i = 0\n",
    "    n = len(raw)\n",
    "\n",
    "    while i < n:\n",
    "        m = DOC_START.search(raw, i)\n",
    "        if not m:\n",
    "            break\n",
    "\n",
    "        m_end = DOC_END.search(raw, m.end())\n",
    "        if not m_end:\n",
    "            break\n",
    "\n",
    "        doc_text = raw[m.end():m_end.start()]  # inside <DOCUMENT>...</DOCUMENT>\n",
    "        i = m_end.end()\n",
    "\n",
    "        \n",
    "        tmatch = TYPE_LINE.search(doc_text)\n",
    "        doc_type = (tmatch.group(1).strip().upper() if tmatch else \"\")\n",
    "\n",
    "        # Skip junk\n",
    "        if doc_type in SKIP_TYPES:\n",
    "            continue\n",
    "        if is_xbrl_aux(doc_type):\n",
    "            continue\n",
    "        if not should_keep_doc_type(doc_type, primary_form):\n",
    "            continue\n",
    "\n",
    "        # Extract <TEXT> if present\n",
    "        upper_doc = doc_text.upper()\n",
    "        t_start = upper_doc.find(\"<TEXT>\")\n",
    "        t_end = upper_doc.rfind(\"</TEXT>\")\n",
    "        if t_start != -1 and t_end != -1 and t_end > t_start:\n",
    "            body = doc_text[t_start + len(\"<TEXT>\"):t_end]\n",
    "        else:\n",
    "            body = doc_text\n",
    "\n",
    "        body = body.strip()\n",
    "\n",
    "        # Drop leading XBRL-ish junk (works even before HTML strip)\n",
    "        body = drop_leading_xbrl_gunk(body)\n",
    "\n",
    "        # Detect HTML-ish content\n",
    "        if bool(re.search(r\"(?i)<\\s*(html|body|div|p|br|table|tr|td|font)\\b\", body)) or \"</\" in body:\n",
    "            body = strip_html_to_text(body)\n",
    "\n",
    "        body = body.strip()\n",
    "        if body:\n",
    "            label = doc_type if doc_type else \"UNKNOWN\"\n",
    "            out_parts.append(f\"----- DOCUMENT TYPE: {label} -----\")\n",
    "            out_parts.append(body)\n",
    "            out_parts.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(out_parts).strip()\n",
    "\n",
    "\n",
    "#  Connect to WRDS\n",
    "db = wrds.Connection()\n",
    "\n",
    "\n",
    "#  Load SEC public paths\n",
    "df = pd.read_csv(\"fname_list.csv\")  # must contain column: path OR fname\n",
    "\n",
    "if \"path\" not in df.columns and \"fname\" in df.columns:\n",
    "    df = df.rename(columns={\"fname\": \"path\"})\n",
    "\n",
    "if \"path\" not in df.columns:\n",
    "    raise ValueError(f\"CSV must contain column 'path' or 'fname'. Found: {list(df.columns)}\")\n",
    "\n",
    "print(\"Mapping SEC public paths → WRDS internal paths...\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Query WRDS mappings\n",
    "public_paths_tuple = tuple(df[\"path\"].unique())\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT fname, wrdsfname\n",
    "    FROM wrdssec.wrds_forms\n",
    "    WHERE fname IN {public_paths_tuple}\n",
    "\"\"\"\n",
    "\n",
    "mapping = db.raw_sql(query)\n",
    "df = df.merge(mapping, how=\"left\", left_on=\"path\", right_on=\"fname\")\n",
    "\n",
    "print(\"Mapping completed.\\n\")\n",
    "\n",
    "\n",
    "# Output directory (CLEAN ver ONLY)\n",
    "out_dir = \"/scratch/oregonstate/kuohsu/filings_text\" # replace kuohsu with your own folder name\n",
    "\n",
    "if os.path.exists(out_dir):\n",
    "    print(f\"Removing existing folder: {out_dir}\")\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "print(f\"Saving CLEAN filings to: {out_dir}\\n\")\n",
    "\n",
    "\n",
    "# Extract from WRDS filesystem → write CLEAN ONLY\n",
    "print(\"Extracting filings from WRDS filesystem...\\n\")\n",
    "\n",
    "ok = 0\n",
    "no_match = 0\n",
    "missing = 0\n",
    "failed = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    wrdsfname = row[\"wrdsfname\"]\n",
    "\n",
    "    if pd.isna(wrdsfname):\n",
    "        print(\"NO MATCH:\", row[\"path\"])\n",
    "        no_match += 1\n",
    "        continue\n",
    "\n",
    "    file_path = f\"/wrds/sec/warchives/{wrdsfname}\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"MISSING:\", file_path)\n",
    "        missing += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", errors=\"ignore\") as f:\n",
    "            raw = f.read()\n",
    "\n",
    "        clean_text = clean_sec_filing(raw, keep_header=True)\n",
    "\n",
    "        # Output name: original basename + _clean.txt\n",
    "        base = os.path.basename(wrdsfname)\n",
    "        if base.lower().endswith(\".txt\"):\n",
    "            out_name = base.replace(\".txt\", \"_clean.txt\")\n",
    "        else:\n",
    "            out_name = base + \"_clean.txt\"\n",
    "\n",
    "        out_path = os.path.join(out_dir, out_name)\n",
    "\n",
    "        with open(out_path, \"w\", errors=\"ignore\") as out:\n",
    "            out.write(clean_text)\n",
    "\n",
    "        ok += 1  # silent success\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"FAILED:\", file_path, e)\n",
    "        failed += 1\n",
    "\n",
    "\n",
    "# Summary\n",
    "print(\"\\nDone!\")\n",
    "print(f\"OK: {ok}\")\n",
    "print(f\"NO MATCH: {no_match}\")\n",
    "print(f\"MISSING INPUT FILE: {missing}\")\n",
    "print(f\"FAILED: {failed}\")\n",
    "print(f\"All CLEAN files saved under: {out_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create tar.gz archive\n",
    "try:\n",
    "    files = [\n",
    "        f for f in os.listdir(out_dir)\n",
    "        if os.path.isfile(os.path.join(out_dir, f))\n",
    "    ]\n",
    "    file_count = len(files)\n",
    "except Exception as e:\n",
    "    print(\"ERROR listing output directory:\", e)\n",
    "    file_count = 0\n",
    "\n",
    "print(f\"\\nOutput file count (clean only): {file_count}\")\n",
    "\n",
    "if file_count == 0:\n",
    "    print(\"ERROR: Output folder is empty. Archive not created.\")\n",
    "else:\n",
    "    parent_dir = os.path.dirname(out_dir)              # /scratch/oregonstate/kuohsu\n",
    "    folder_name = os.path.basename(out_dir)            # filings_text\n",
    "    base_name = os.path.join(parent_dir, folder_name)  # /scratch/.../filings_text\n",
    "\n",
    "    tar_gz_path = base_name + \".tar.gz\"\n",
    "\n",
    "    if os.path.exists(tar_gz_path):\n",
    "        print(f\"Removing existing archive: {tar_gz_path}\")\n",
    "        os.remove(tar_gz_path)\n",
    "\n",
    "    print(f\"Creating archive: {tar_gz_path}\")\n",
    "    shutil.make_archive(\n",
    "        base_name,\n",
    "        format=\"gztar\",\n",
    "        root_dir=parent_dir,\n",
    "        base_dir=folder_name\n",
    "    )\n",
    "\n",
    "    size_mb = os.path.getsize(tar_gz_path) / (1024 * 1024)\n",
    "    print(f\"Archive created ({size_mb:.2f} MB)\")\n",
    "\n",
    "\n",
    "\n",
    "# final step: compressed file download\n",
    "# Create ZIP files\n",
    "#   - Single:  filings_text.zip\n",
    "#   - Split:   filings_text.z01, filings_text.z02, ... , filings_text.zip\n",
    "#   - Extract on Windows: keep all parts in same folder, open filings_text.zip\n",
    "if file_count != 0:\n",
    "    PART_MB = 3000  # 3GB per part\n",
    "\n",
    "    parent_dir = os.path.dirname(out_dir)\n",
    "    folder_name = os.path.basename(out_dir)  # filings_text\n",
    "    zip_base = os.path.join(parent_dir, folder_name)  # /scratch/.../filings_text\n",
    "    zip_path = zip_base + \".zip\"                      # /scratch/.../filings_text.zip\n",
    "\n",
    "    # check zip exists\n",
    "    if shutil.which(\"zip\") is None:\n",
    "        print(\"\\nERROR: 'zip' command not found on WRDS.\")\n",
    "        print(\"Fallback: use tar.gz + .part001 splitting (your old method).\")\n",
    "        raise RuntimeError(\"zip not installed\")\n",
    "\n",
    "    # remove old split-zip outputs\n",
    "    for old in glob.glob(zip_base + \".z[0-9][0-9]\") + glob.glob(zip_path):\n",
    "        try:\n",
    "            os.remove(old)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(f\"\\nCreating split ZIP (~{PART_MB} MB per part if needed)...\")\n",
    "\n",
    "\n",
    "    \n",
    "    cmd = [\"zip\", \"-r\", f\"-s{PART_MB}m\", zip_path, folder_name]\n",
    "    \n",
    "    subprocess.run(\n",
    "        cmd,\n",
    "        cwd=parent_dir,\n",
    "        check=True,\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    \n",
    "    print(\"\\nZIP creation finished.\")\n",
    "\n",
    "    zip_parts = sorted(glob.glob(zip_base + \".z[0-9][0-9]\"))\n",
    "    all_zip_files = zip_parts + ([zip_path] if os.path.exists(zip_path) else [])\n",
    "\n",
    "    print(f\"Created {len(all_zip_files)} ZIP file(s):\")\n",
    "    for p in all_zip_files:\n",
    "        print(\" -\", p)\n",
    "\n",
    "    # stage to home one by one\n",
    "    home_dir = os.path.expanduser(\"~\")\n",
    "\n",
    "    for p in all_zip_files:\n",
    "        name = os.path.basename(p)\n",
    "        home_p = os.path.join(home_dir, name)\n",
    "\n",
    "        if os.path.exists(home_p):\n",
    "            try:\n",
    "                os.remove(home_p)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        print(f\"\\nCopying {name} to home: {home_p}\")\n",
    "        shutil.copy2(p, home_p)\n",
    "\n",
    "        size_mb = os.path.getsize(home_p) / (1024 * 1024)\n",
    "        print(f\" Staged {name} ({size_mb:.2f} MB) in home directory.\")\n",
    "        print(\" Download it now (Jupyter UI download or scp from your local machine).\")\n",
    "        print(\" IMPORTANT: AFTER downloading to your local device, delete it from home to free the quota.\")\n",
    "\n",
    "        # user instructions\n",
    "        if len(all_zip_files) == 1 and name.endswith(\".zip\"):\n",
    "            print(\"\\nNOTE: Single ZIP file (not split). You can extract directly in WinRAR/7-Zip.\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nNOTE: Split ZIP set for WinRAR/7-Zip:\")\n",
    "            print(\" - Download ALL .z01/.z02/... AND the .zip into the SAME folder.\")\n",
    "            print(\" - Open ONLY the .zip file (e.g., filings_text.zip) to extract.\")\n",
    "            print(\"   (Do NOT open .z01/.z02 directly.)\")\n",
    "\n",
    "        while True:\n",
    "            ans = input(\"Type 'ok' after you downloaded AND deleted the file from home: \").strip().lower()\n",
    "            if ans == \"ok\":\n",
    "                break\n",
    "            print(\"Please type exactly 'ok' to continue.\")\n",
    "        \n",
    "        print(\"\\nTask Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08c712-b7f2-4633-9164-a29f8c1a93d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
