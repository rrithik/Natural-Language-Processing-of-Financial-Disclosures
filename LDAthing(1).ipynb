{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens after cleaning: 38540\n",
            "Sample tokens: ['document', 'txt', 'header', 'hdr', 'sgml', 'acceptance', 'datetime', 'accession', 'number', 'conform', 'submission', 'type', 's', 'public', 'document', 'count', 'fil', 'date', 'date', 'change', 'filer', 'data', 'conform', 'name', 'walt', 'disney', 'co', 'central', 'index', 'key', 'standard', 'industrial', 'classification', 'service', 'miscellaneou', 'amusement', 'recreation', 'irs', 'number', 'state']\n",
            "\n",
            "Top 30 most common words:\n",
            "note: 2916\n",
            "exchange: 1205\n",
            "due: 1064\n",
            "any: 777\n",
            "date: 609\n",
            "amount: 586\n",
            "original: 570\n",
            "principal: 567\n",
            "interest: 512\n",
            "securitie: 489\n",
            "offer: 380\n",
            "act: 317\n",
            "payment: 306\n",
            "aggregate: 304\n",
            "holder: 301\n",
            "register: 292\n",
            "other: 285\n",
            "are: 257\n",
            "disney: 240\n",
            "indenture: 222\n",
            "all: 208\n",
            "registration: 201\n",
            "serie: 196\n",
            "tender: 195\n",
            "redemption: 194\n",
            "its: 193\n",
            "prospectu: 181\n",
            "transfer: 181\n",
            "includ: 176\n",
            "s: 164\n",
            "\n",
            "ASCII Word Cloud:\n",
            "note            ##################################################\n",
            "exchange        ##################################################\n",
            "due             ##################################################\n",
            "any             ##################################################\n",
            "date            ##################################################\n",
            "amount          ##################################################\n",
            "original        ##################################################\n",
            "principal       ##################################################\n",
            "interest        ##################################################\n",
            "securitie       ##################################################\n",
            "offer           ##################################################\n",
            "act             ##################################################\n",
            "payment         ##################################################\n",
            "aggregate       ##################################################\n",
            "holder          ##################################################\n",
            "register        ##################################################\n",
            "other           ##################################################\n",
            "are             ##################################################\n",
            "disney          ##################################################\n",
            "indenture       ##################################################\n",
            "all             ##################################################\n",
            "registration    ##################################################\n",
            "serie           ##################################################\n",
            "tender          ##################################################\n",
            "redemption      ##################################################\n",
            "its             ##################################################\n",
            "prospectu       ##################################################\n",
            "transfer        ##################################################\n",
            "includ          ##################################################\n",
            "s               ##################################################\n",
            "\n",
            "Built 193 pseudo-documents (effective chunk size ~200).\n",
            "\n",
            "Running Gibbs sampling...\n",
            "Iteration 1/400\n",
            "Iteration 100/400\n",
            "Iteration 200/400\n",
            "Iteration 300/400\n",
            "Iteration 400/400\n",
            "Gibbs sampling finished.\n",
            "\n",
            "Topics (top words with weights):\n",
            "\n",
            "Topic 1:\n",
            "0.070 exchange\n",
            "0.059 note\n",
            "0.043 original\n",
            "0.036 offer\n",
            "0.027 any\n",
            "0.022 tender\n",
            "0.016 prospectu\n",
            "0.014 letter\n",
            "0.013 are\n",
            "0.011 holder\n",
            "0.010 registration\n",
            "0.009 securitie\n",
            "0.009 date\n",
            "0.009 connection\n",
            "0.008 other\n",
            "\n",
            "Topic 2:\n",
            "0.273 note\n",
            "0.183 due\n",
            "0.084 amount\n",
            "0.075 principal\n",
            "0.053 aggregate\n",
            "0.050 exchange\n",
            "0.036 original\n",
            "0.033 securitie\n",
            "0.033 act\n",
            "0.033 register\n",
            "0.016 like\n",
            "0.012 offer\n",
            "0.008 u\n",
            "0.007 equal\n",
            "0.007 equivalent\n",
            "\n",
            "Topic 3:\n",
            "0.021 disney\n",
            "0.018 document\n",
            "0.018 twdc\n",
            "0.018 registration\n",
            "0.018 s\n",
            "0.018 report\n",
            "0.017 enterprise\n",
            "0.017 fil\n",
            "0.017 securitie\n",
            "0.016 information\n",
            "0.015 walt\n",
            "0.015 reference\n",
            "0.014 act\n",
            "0.013 incorporat\n",
            "0.012 exhibit\n",
            "\n",
            "Topic 4:\n",
            "0.094 date\n",
            "0.093 note\n",
            "0.072 interest\n",
            "0.059 exchange\n",
            "0.042 payment\n",
            "0.035 redemption\n",
            "0.025 any\n",
            "0.019 first\n",
            "0.016 maturity\n",
            "0.016 list\n",
            "0.014 rate\n",
            "0.013 amount\n",
            "0.013 principal\n",
            "0.012 record\n",
            "0.011 after\n",
            "\n",
            "Topic 5:\n",
            "0.023 other\n",
            "0.023 any\n",
            "0.018 disney\n",
            "0.016 law\n",
            "0.016 person\n",
            "0.016 guarantor\n",
            "0.012 interest\n",
            "0.011 are\n",
            "0.011 subsidiarie\n",
            "0.010 includ\n",
            "0.010 our\n",
            "0.009 asset\n",
            "0.008 than\n",
            "0.008 share\n",
            "0.008 federal\n",
            "\n",
            "Topic 6:\n",
            "0.039 note\n",
            "0.029 any\n",
            "0.022 indenture\n",
            "0.020 holder\n",
            "0.014 trustee\n",
            "0.013 shall\n",
            "0.012 transfer\n",
            "0.012 new\n",
            "0.012 agent\n",
            "0.012 serie\n",
            "0.012 its\n",
            "0.011 all\n",
            "0.010 york\n",
            "0.010 other\n",
            "0.009 dtc\n",
            "\n",
            "Topics formatted for prompt input (comma-separated keywords):\n",
            "Topic 1: exchange, note, original, offer, any, tender, prospectu, letter\n",
            "Topic 2: note, due, amount, principal, aggregate, exchange, original, securitie\n",
            "Topic 3: disney, document, twdc, registration, s, report, enterprise, fil\n",
            "Topic 4: date, note, interest, exchange, payment, redemption, any, first\n",
            "Topic 5: other, any, disney, law, person, guarantor, interest, are\n",
            "Topic 6: note, any, indenture, holder, trustee, shall, transfer, new\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Offline cleaning utilities\n",
        "# ----------------------------\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic lowercase + non-letter removal. Offline and lightweight on purpose.\n",
        "    Design note: We can swap this out for something NLTK/spaCy-based in the future.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Base stopword list (general English).\n",
        "simple_stopwords = set(\"\"\"\n",
        "a an the and or if in on at to for of this that is was were be been it with as by from into up out \n",
        "do does did doing have has had not so such these those he she they them we you i but which will would\n",
        "should can could may might also about over under between during per each eachother\n",
        "\"\"\".split())\n",
        "\n",
        "# Design: domain-specific stopwords for SEC / financial disclosures.\n",
        "# Extend this set as you learn more about the structure & syntax of your filings.\n",
        "domain_stopwords = set([\n",
        "    # Examples / placeholders â€“ add real disclosure-specific terms here over time:\n",
        "    \"inc\", \"ltd\", \"corp\", \"company\", \"llc\", \"plc\",\n",
        "    \"form\", \"sec\", \"statement\"\n",
        "])\n",
        "\n",
        "# Merge base + domain-specific stopwords\n",
        "simple_stopwords |= domain_stopwords\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [t for t in tokens if t not in simple_stopwords]\n",
        "\n",
        "def perform_lemmatization(tokens):\n",
        "    \"\"\"Very simple rule-based lemmatization.\n",
        "    Design note: This is intentionally lightweight and offline.\n",
        "    In the future, this function is the hook where we could plug in NLTK/spaCy\n",
        "    lemmatization for better accuracy/efficiency.\"\"\"\n",
        "    lemmas = []\n",
        "    for t in tokens:\n",
        "        if t.endswith(\"ing\") and len(t) > 4:\n",
        "            lemmas.append(t[:-3])\n",
        "        elif t.endswith(\"ed\") and len(t) > 3:\n",
        "            lemmas.append(t[:-2])\n",
        "        elif t.endswith(\"s\") and len(t) > 3:\n",
        "            lemmas.append(t[:-1])\n",
        "        else:\n",
        "            lemmas.append(t)\n",
        "    return lemmas\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = perform_lemmatization(tokens)\n",
        "    return tokens\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Load file (change filename if needed)\n",
        "# ----------------------------\n",
        "FILENAME = \"Disney.txt\"  # change if your file has another name\n",
        "\n",
        "with open(FILENAME, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw = f.read()\n",
        "\n",
        "tokens = clean_text(raw)\n",
        "\n",
        "# Basic diagnostics\n",
        "print(f\"Total tokens after cleaning: {len(tokens)}\")\n",
        "print(\"Sample tokens:\", tokens[:40])\n",
        "\n",
        "if not tokens:\n",
        "    print(\"No tokens after cleaning; nothing to model.\")\n",
        "    raise SystemExit\n",
        "\n",
        "# Frequency and ASCII word cloud (top 30)\n",
        "freq = Counter(tokens)\n",
        "most_common_30 = freq.most_common(30)\n",
        "\n",
        "print(\"\\nTop 30 most common words:\")\n",
        "for w, c in most_common_30:\n",
        "    print(f\"{w}: {c}\")\n",
        "\n",
        "print(\"\\nASCII Word Cloud:\")\n",
        "max_width = 50\n",
        "for word, count in most_common_30:\n",
        "    bar = \"#\" * min(count, max_width)\n",
        "    print(f\"{word:15} {bar}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Build pseudo-documents\n",
        "#    Design/correctness: Only chunk large disclosures; small ones stay as a single doc.\n",
        "# ----------------------------\n",
        "TOTAL_TOKENS = len(tokens)\n",
        "\n",
        "# \"Target\" chunk size for large disclosures\n",
        "DOC_CHUNK_SIZE = 200\n",
        "\n",
        "# Threshold: below this, treat as one document instead of forcing multiple small chunks\n",
        "MIN_TOKENS_FOR_MULTI_DOC = 800  # design parameter you can tune\n",
        "\n",
        "if TOTAL_TOKENS == 0:\n",
        "    documents = []\n",
        "elif TOTAL_TOKENS < MIN_TOKENS_FOR_MULTI_DOC:\n",
        "    # Small/medium disclosure: keep as a single document\n",
        "    documents = [tokens]\n",
        "else:\n",
        "    # Large disclosure: chunk into pseudo-documents\n",
        "    documents = []\n",
        "    for i in range(0, TOTAL_TOKENS, DOC_CHUNK_SIZE):\n",
        "        documents.append(tokens[i: i + DOC_CHUNK_SIZE])\n",
        "\n",
        "    # If we still ended up with very few docs, slightly reduce chunk size\n",
        "    # to get a bit more doc-level variation for LDA.\n",
        "    if len(documents) < 5:\n",
        "        DOC_CHUNK_SIZE = max(50, TOTAL_TOKENS // 8)\n",
        "        documents = [tokens[i: i + DOC_CHUNK_SIZE]\n",
        "                     for i in range(0, TOTAL_TOKENS, DOC_CHUNK_SIZE)]\n",
        "\n",
        "num_docs = len(documents)\n",
        "print(f\"\\nBuilt {num_docs} pseudo-documents (effective chunk size ~{DOC_CHUNK_SIZE}).\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Prepare vocabulary and ids\n",
        "# ----------------------------\n",
        "vocab = sorted(list(set(tokens)))\n",
        "V = len(vocab)\n",
        "word2id = {w: i for i, w in enumerate(vocab)}\n",
        "id2word = {i: w for w, i in word2id.items()}\n",
        "\n",
        "# Convert documents to id-lists\n",
        "docs_wids = [[word2id[w] for w in doc] for doc in documents]\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Simple collapsed Gibbs sampler for LDA\n",
        "# ----------------------------\n",
        "NUM_TOPICS = 6         # change number of topics\n",
        "ALPHA = 0.1            # doc-topic prior\n",
        "BETA = 0.01            # topic-word prior\n",
        "ITERATIONS = 400       # sampling iterations (increase for better convergence)\n",
        "TOP_N_WORDS = 15       # words to print per topic\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "D = len(docs_wids)\n",
        "K = NUM_TOPICS\n",
        "\n",
        "# counts\n",
        "doc_topic_counts = [defaultdict(int) for _ in range(D)]\n",
        "topic_word_counts = [defaultdict(int) for _ in range(K)]\n",
        "topic_counts = [0 for _ in range(K)]\n",
        "doc_lengths = [len(doc) for doc in docs_wids]\n",
        "\n",
        "# initial topic assignments (random)\n",
        "topic_assignments = []\n",
        "for d, doc in enumerate(docs_wids):\n",
        "    current_doc_topics = []\n",
        "    for w in doc:\n",
        "        z = random.randrange(K)\n",
        "        current_doc_topics.append(z)\n",
        "        doc_topic_counts[d][z] += 1\n",
        "        topic_word_counts[z][w] += 1\n",
        "        topic_counts[z] += 1\n",
        "    topic_assignments.append(current_doc_topics)\n",
        "\n",
        "# Precompute constants\n",
        "V_beta = V * BETA\n",
        "\n",
        "def sample_topic(d, w, current_z):\n",
        "    # remove current assignment\n",
        "    doc_topic_counts[d][current_z] -= 1\n",
        "    if doc_topic_counts[d][current_z] == 0:\n",
        "        del doc_topic_counts[d][current_z]\n",
        "    topic_word_counts[current_z][w] -= 1\n",
        "    if topic_word_counts[current_z][w] == 0:\n",
        "        del topic_word_counts[current_z][w]\n",
        "    topic_counts[current_z] -= 1\n",
        "\n",
        "    # compute full conditional for each topic\n",
        "    probs = []\n",
        "    for k in range(K):\n",
        "        left = (doc_topic_counts[d].get(k, 0) + ALPHA) / (doc_lengths[d] - 1 + K * ALPHA)\n",
        "        right = (topic_word_counts[k].get(w, 0) + BETA) / (topic_counts[k] + V_beta)\n",
        "        probs.append(left * right)\n",
        "\n",
        "    # normalize\n",
        "    s = sum(probs)\n",
        "    if s == 0:\n",
        "        probs = [1.0 / K] * K\n",
        "    else:\n",
        "        probs = [p / s for p in probs]\n",
        "\n",
        "    # draw new topic\n",
        "    r = random.random()\n",
        "    cum = 0.0\n",
        "    new_z = 0\n",
        "    for k, p in enumerate(probs):\n",
        "        cum += p\n",
        "        if r <= cum:\n",
        "            new_z = k\n",
        "            break\n",
        "\n",
        "    # add back new assignment\n",
        "    doc_topic_counts[d][new_z] += 1\n",
        "    topic_word_counts[new_z][w] += 1\n",
        "    topic_counts[new_z] += 1\n",
        "\n",
        "    return new_z\n",
        "\n",
        "# Gibbs sampling iterations\n",
        "print(\"\\nRunning Gibbs sampling...\")\n",
        "for it in range(1, ITERATIONS + 1):\n",
        "    for d, doc in enumerate(docs_wids):\n",
        "        for i, w in enumerate(doc):\n",
        "            current_z = topic_assignments[d][i]\n",
        "            new_z = sample_topic(d, w, current_z)\n",
        "            topic_assignments[d][i] = new_z\n",
        "\n",
        "    if it % 100 == 0 or it == 1:\n",
        "        print(f\"Iteration {it}/{ITERATIONS}\")\n",
        "\n",
        "print(\"Gibbs sampling finished.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Compute topic-word distributions (phi) and print topics.\n",
        "# ----------------------------\n",
        "phi = []\n",
        "for k in range(K):\n",
        "    phi_k = {}\n",
        "    denom = topic_counts[k] + V_beta\n",
        "    if denom == 0:\n",
        "        phi.append(phi_k)\n",
        "        continue\n",
        "    for w_id in topic_word_counts[k]:\n",
        "        phi_k[w_id] = (topic_word_counts[k].get(w_id, 0) + BETA) / denom\n",
        "    phi.append(phi_k)\n",
        "\n",
        "print(\"\\nTopics (top words with weights):\")\n",
        "for k in range(K):\n",
        "    if not phi[k]:\n",
        "        print(f\"\\nTopic {k+1}: (empty)\")\n",
        "        continue\n",
        "\n",
        "    word_probs = [(id2word[w_id], prob) for w_id, prob in phi[k].items()]\n",
        "    word_probs.sort(key=lambda x: x[1], reverse=True)\n",
        "    top = word_probs[:TOP_N_WORDS]\n",
        "\n",
        "    print(f\"\\nTopic {k+1}:\")\n",
        "    for word, prob in top:\n",
        "        print(f\"{prob:.3f} {word}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Prompt-friendly topic output for AI usage\n",
        "# ----------------------------\n",
        "print(\"\\nTopics formatted for prompt input (comma-separated keywords):\")\n",
        "for k in range(K):\n",
        "    if not phi[k]:\n",
        "        print(f\"Topic {k+1}: (empty)\")\n",
        "        continue\n",
        "    top_words = sorted(phi[k].items(), key=lambda x: x[1], reverse=True)[:8]\n",
        "    words_only = [id2word[w_id] for w_id, _ in top_words]\n",
        "    print(f\"Topic {k+1}: {', '.join(words_only)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
