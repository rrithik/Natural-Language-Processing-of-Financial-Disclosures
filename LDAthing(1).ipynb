{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens after cleaning: 1870\n",
            "Sample tokens: ['sec', 'document', 'txt', 'sec', 'header', 'hdr', 'sgml', 'acceptance', 'datetime', 'accession', 'number', 'conform', 'submission', 'type', 'k', 'public', 'document', 'count', 'conform', 'period', 'report', 'item', 'information', 'other', 'event', 'fil', 'date', 'date', 'change', 'filer', 'company', 'data', 'company', 'conform', 'name', 'nike', 'inc', 'central', 'index', 'key']\n",
            "\n",
            "Top 30 most common words:\n",
            "type: 92\n",
            "dei: 56\n",
            "name: 52\n",
            "document: 47\n",
            "definition: 36\n",
            "reference: 33\n",
            "entity: 32\n",
            "number: 29\n",
            "period: 29\n",
            "exchange: 27\n",
            "act: 26\n",
            "data: 24\n",
            "text: 24\n",
            "prefix: 24\n",
            "xbrl: 23\n",
            "x: 23\n",
            "detail: 23\n",
            "namespace: 23\n",
            "balance: 23\n",
            "na: 23\n",
            "duration: 23\n",
            "sec: 21\n",
            "addres: 20\n",
            "false: 20\n",
            "report: 19\n",
            "b: 19\n",
            "fil: 18\n",
            "k: 17\n",
            "company: 15\n",
            "nke: 15\n",
            "\n",
            "ASCII Word Cloud:\n",
            "type            ##################################################\n",
            "dei             ##################################################\n",
            "name            ##################################################\n",
            "document        ###############################################\n",
            "definition      ####################################\n",
            "reference       #################################\n",
            "entity          ################################\n",
            "number          #############################\n",
            "period          #############################\n",
            "exchange        ###########################\n",
            "act             ##########################\n",
            "data            ########################\n",
            "text            ########################\n",
            "prefix          ########################\n",
            "xbrl            #######################\n",
            "x               #######################\n",
            "detail          #######################\n",
            "namespace       #######################\n",
            "balance         #######################\n",
            "na              #######################\n",
            "duration        #######################\n",
            "sec             #####################\n",
            "addres          ####################\n",
            "false           ####################\n",
            "report          ###################\n",
            "b               ###################\n",
            "fil             ##################\n",
            "k               #################\n",
            "company         ###############\n",
            "nke             ###############\n",
            "\n",
            "Built 10 pseudo-documents (chunk size ~200).\n",
            "\n",
            "Running Gibbs sampling...\n",
            "Iteration 1/400\n",
            "Iteration 100/400\n",
            "Iteration 200/400\n",
            "Iteration 300/400\n",
            "Iteration 400/400\n",
            "Gibbs sampling finished.\n",
            "\n",
            "Topics (top words with weights):\n",
            "\n",
            "Topic 1:\n",
            "0.153 type\n",
            "0.080 name\n",
            "0.074 definition\n",
            "0.068 reference\n",
            "0.052 period\n",
            "0.047 na\n",
            "0.047 x\n",
            "0.047 duration\n",
            "0.047 balance\n",
            "0.047 detail\n",
            "0.035 namespace\n",
            "0.027 available\n",
            "0.023 number\n",
            "0.023 role\n",
            "0.021 presentationref\n",
            "\n",
            "Topic 2:\n",
            "0.177 dei\n",
            "0.077 data\n",
            "0.077 prefix\n",
            "0.052 sec\n",
            "0.048 no\n",
            "0.045 xbrli\n",
            "0.039 xbrl\n",
            "0.039 http\n",
            "0.039 act\n",
            "0.035 www\n",
            "0.032 publisher\n",
            "0.029 subsection\n",
            "0.023 form\n",
            "0.019 submission\n",
            "0.019 booleanitemtype\n",
            "\n",
            "Topic 3:\n",
            "0.140 document\n",
            "0.072 text\n",
            "0.060 addres\n",
            "0.060 false\n",
            "0.054 type\n",
            "0.045 nke\n",
            "0.036 description\n",
            "0.036 filename\n",
            "0.036 sequence\n",
            "0.036 xml\n",
            "0.033 xbrl\n",
            "0.027 cover\n",
            "0.024 one\n",
            "0.024 nike\n",
            "0.021 idea\n",
            "\n",
            "Topic 4:\n",
            "0.120 entity\n",
            "0.049 b\n",
            "0.049 pre\n",
            "0.038 commencement\n",
            "0.034 communication\n",
            "0.034 security\n",
            "0.034 exchange\n",
            "0.030 e\n",
            "0.030 registrant\n",
            "0.026 emerg\n",
            "0.026 growth\n",
            "0.026 code\n",
            "0.023 offer\n",
            "0.023 tender\n",
            "0.019 solicit\n",
            "\n",
            "Topic 5:\n",
            "0.074 number\n",
            "0.054 name\n",
            "0.054 date\n",
            "0.054 state\n",
            "0.041 k\n",
            "0.037 city\n",
            "0.037 zip\n",
            "0.033 code\n",
            "0.025 incorporation\n",
            "0.025 identification\n",
            "0.025 file\n",
            "0.021 phone\n",
            "0.021 area\n",
            "0.021 beaverton\n",
            "0.021 end\n",
            "\n",
            "Topic 6:\n",
            "0.059 report\n",
            "0.055 act\n",
            "0.055 exchange\n",
            "0.051 company\n",
            "0.047 pursuant\n",
            "0.047 fil\n",
            "0.039 rule\n",
            "0.036 securitie\n",
            "0.028 k\n",
            "0.028 registrant\n",
            "0.024 form\n",
            "0.020 c\n",
            "0.020 purchase\n",
            "0.020 obligation\n",
            "0.016 annual\n",
            "\n",
            "Top words per topic (concise):\n",
            "Topic 1: type, name, definition, reference, period, na, x, duration\n",
            "Topic 2: dei, data, prefix, sec, no, xbrli, xbrl, http\n",
            "Topic 3: document, text, addres, false, type, nke, description, filename\n",
            "Topic 4: entity, b, pre, commencement, communication, security, exchange, e\n",
            "Topic 5: number, name, date, state, k, city, zip, code\n",
            "Topic 6: report, act, exchange, company, pursuant, fil, rule, securitie\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Offline cleaning utilities\n",
        "# ----------------------------\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "simple_stopwords = set(\"\"\"\n",
        "a an the and or if in on at to for of this that is was were be been it with as by from into up out \n",
        "do does did doing have has had not so such these those he she they them we you i but which will would\n",
        "should can could may might also about over under between during per each eachother\n",
        "\"\"\".split())\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [t for t in tokens if t not in simple_stopwords]\n",
        "\n",
        "def perform_lemmatization(tokens):\n",
        "    lemmas = []\n",
        "    for t in tokens:\n",
        "        if t.endswith(\"ing\") and len(t) > 4:\n",
        "            lemmas.append(t[:-3])\n",
        "        elif t.endswith(\"ed\") and len(t) > 3:\n",
        "            lemmas.append(t[:-2])\n",
        "        elif t.endswith(\"s\") and len(t) > 3:\n",
        "            lemmas.append(t[:-1])\n",
        "        else:\n",
        "            lemmas.append(t)\n",
        "    return lemmas\n",
        "\n",
        "def clean_text(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = perform_lemmatization(tokens)\n",
        "    return tokens\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Load file (change filename if needed)\n",
        "# ----------------------------\n",
        "FILENAME = \"0000320187-25-000053.txt\"  # change if your file has another name\n",
        "\n",
        "with open(FILENAME, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw = f.read()\n",
        "\n",
        "tokens = clean_text(raw)\n",
        "\n",
        "# Print the same basic diagnostics you showed earlier:\n",
        "print(f\"Total tokens after cleaning: {len(tokens)}\")\n",
        "print(\"Sample tokens:\", tokens[:40])\n",
        "\n",
        "# Frequency and ASCII word cloud (top 30)\n",
        "freq = Counter(tokens)\n",
        "most_common_30 = freq.most_common(30)\n",
        "\n",
        "print(\"\\nTop 30 most common words:\")\n",
        "for w, c in most_common_30:\n",
        "    print(f\"{w}: {c}\")\n",
        "\n",
        "print(\"\\nASCII Word Cloud:\")\n",
        "max_width = 50\n",
        "for word, count in most_common_30:\n",
        "    bar = \"#\" * min(count, max_width)\n",
        "    print(f\"{word:15} {bar}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Build pseudo-documents\n",
        "# If you have true documents, replace this step and provide a list of token lists.\n",
        "# Here we chunk the token list into multiple small documents to run LDA.\n",
        "# ----------------------------\n",
        "TOTAL_TOKENS = len(tokens)\n",
        "# Choose number of pseudo-documents (roughly one doc per 150-300 tokens)\n",
        "DOC_CHUNK_SIZE = 200\n",
        "if TOTAL_TOKENS <= DOC_CHUNK_SIZE:\n",
        "    documents = [tokens]   # single short doc\n",
        "else:\n",
        "    documents = []\n",
        "    for i in range(0, TOTAL_TOKENS, DOC_CHUNK_SIZE):\n",
        "        documents.append(tokens[i : i + DOC_CHUNK_SIZE])\n",
        "\n",
        "# Optionally: make number of documents at least 5 for stable topics\n",
        "if len(documents) < 5 and TOTAL_TOKENS > 0:\n",
        "    # re-chunk with smaller chunk size\n",
        "    DOC_CHUNK_SIZE = max(20, TOTAL_TOKENS // 8)\n",
        "    documents = [tokens[i : i + DOC_CHUNK_SIZE] for i in range(0, TOTAL_TOKENS, DOC_CHUNK_SIZE)]\n",
        "\n",
        "num_docs = len(documents)\n",
        "print(f\"\\nBuilt {num_docs} pseudo-documents (chunk size ~{DOC_CHUNK_SIZE}).\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Prepare vocabulary and ids\n",
        "# ----------------------------\n",
        "vocab = sorted(list(set(tokens)))\n",
        "V = len(vocab)\n",
        "word2id = {w: i for i, w in enumerate(vocab)}\n",
        "id2word = {i: w for w, i in word2id.items()}\n",
        "\n",
        "# Convert documents to id-lists\n",
        "docs_wids = [[word2id[w] for w in doc] for doc in documents]\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Simple collapsed Gibbs sampler for LDA\n",
        "# ----------------------------\n",
        "NUM_TOPICS = 6         # change number of topics\n",
        "ALPHA = 0.1            # doc-topic prior\n",
        "BETA = 0.01            # topic-word prior\n",
        "ITERATIONS = 400       # sampling iterations (increase for better convergence)\n",
        "TOP_N_WORDS = 15       # words to print per topic\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "D = len(docs_wids)\n",
        "K = NUM_TOPICS\n",
        "\n",
        "# counts\n",
        "doc_topic_counts = [defaultdict(int) for _ in range(D)]   # ndk\n",
        "topic_word_counts = [defaultdict(int) for _ in range(K)]  # nkw\n",
        "topic_counts = [0 for _ in range(K)]                      # nk\n",
        "doc_lengths = [len(doc) for doc in docs_wids]\n",
        "\n",
        "# initial topic assignments (random)\n",
        "topic_assignments = []\n",
        "for d, doc in enumerate(docs_wids):\n",
        "    current_doc_topics = []\n",
        "    for w in doc:\n",
        "        z = random.randrange(K)\n",
        "        current_doc_topics.append(z)\n",
        "        doc_topic_counts[d][z] += 1\n",
        "        topic_word_counts[z][w] += 1\n",
        "        topic_counts[z] += 1\n",
        "    topic_assignments.append(current_doc_topics)\n",
        "\n",
        "# Precompute constants\n",
        "V_beta = V * BETA\n",
        "\n",
        "def sample_topic(d, w, current_z):\n",
        "    # remove current assignment\n",
        "    doc_topic_counts[d][current_z] -= 1\n",
        "    if doc_topic_counts[d][current_z] == 0:\n",
        "        del doc_topic_counts[d][current_z]\n",
        "    topic_word_counts[current_z][w] -= 1\n",
        "    if topic_word_counts[current_z][w] == 0:\n",
        "        del topic_word_counts[current_z][w]\n",
        "    topic_counts[current_z] -= 1\n",
        "\n",
        "    # compute full conditional for each topic\n",
        "    probs = []\n",
        "    for k in range(K):\n",
        "        left = (doc_topic_counts[d].get(k, 0) + ALPHA) / (doc_lengths[d] - 1 + K * ALPHA)\n",
        "        right = (topic_word_counts[k].get(w, 0) + BETA) / (topic_counts[k] + V_beta)\n",
        "        probs.append(left * right)\n",
        "\n",
        "    # normalize\n",
        "    s = sum(probs)\n",
        "    if s == 0:\n",
        "        probs = [1.0 / K] * K\n",
        "    else:\n",
        "        probs = [p / s for p in probs]\n",
        "\n",
        "    # draw new topic\n",
        "    r = random.random()\n",
        "    cum = 0.0\n",
        "    for k, p in enumerate(probs):\n",
        "        cum += p\n",
        "        if r <= cum:\n",
        "            new_z = k\n",
        "            break\n",
        "\n",
        "    # add back new assignment\n",
        "    doc_topic_counts[d][new_z] += 1\n",
        "    topic_word_counts[new_z][w] += 1\n",
        "    topic_counts[new_z] += 1\n",
        "\n",
        "    return new_z\n",
        "\n",
        "# Gibbs sampling iterations\n",
        "print(\"\\nRunning Gibbs sampling...\")\n",
        "for it in range(1, ITERATIONS + 1):\n",
        "    for d, doc in enumerate(docs_wids):\n",
        "        for i, w in enumerate(doc):\n",
        "            current_z = topic_assignments[d][i]\n",
        "            new_z = sample_topic(d, w, current_z)\n",
        "            topic_assignments[d][i] = new_z\n",
        "\n",
        "    if it % 100 == 0 or it == 1:\n",
        "        print(f\"Iteration {it}/{ITERATIONS}\")\n",
        "\n",
        "print(\"Gibbs sampling finished.\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Compute topic-word distributions (phi) and print topics\n",
        "# ----------------------------\n",
        "# phi[k][w] = (nkw + beta) / (nk + V*beta)\n",
        "phi = []\n",
        "for k in range(K):\n",
        "    phi_k = {}\n",
        "    denom = topic_counts[k] + V_beta\n",
        "    for w_id in topic_word_counts[k]:\n",
        "        phi_k[w_id] = (topic_word_counts[k].get(w_id, 0) + BETA) / denom\n",
        "    # include zeros for unseen words if needed (not necessary for top words)\n",
        "    phi.append(phi_k)\n",
        "\n",
        "# Print topics in requested format (topic index + top words with weights)\n",
        "print(\"\\nTopics (top words with weights):\")\n",
        "for k in range(K):\n",
        "    # get top words for topic k by phi\n",
        "    # If topic has no words (rare), skip\n",
        "    if not phi[k]:\n",
        "        print(f\"\\nTopic {k+1}: (empty)\")\n",
        "        continue\n",
        "\n",
        "    # create list of (word, prob)\n",
        "    word_probs = [(id2word[w_id], prob) for w_id, prob in phi[k].items()]\n",
        "    word_probs.sort(key=lambda x: x[1], reverse=True)\n",
        "    top = word_probs[:TOP_N_WORDS]\n",
        "\n",
        "    print(f\"\\nTopic {k+1}:\")\n",
        "    for prob_word in top:\n",
        "        prob = prob_word[1]\n",
        "        word = prob_word[0]\n",
        "        # print weight with 3 decimals like \"0.123 word\"\n",
        "        print(f\"{prob:.3f} {word}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Optionally, show topic-word counts for inspection\n",
        "# ----------------------------\n",
        "# Example: most probable words across topics (concise)\n",
        "print(\"\\nTop words per topic (concise):\")\n",
        "for k in range(K):\n",
        "    if not phi[k]:\n",
        "        print(f\"Topic {k+1}: (empty)\")\n",
        "        continue\n",
        "    top_words = sorted(phi[k].items(), key=lambda x: x[1], reverse=True)[:8]\n",
        "    print(\"Topic %d:\" % (k+1), \", \".join([f\"{id2word[w_id]}\" for w_id, _ in top_words]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
