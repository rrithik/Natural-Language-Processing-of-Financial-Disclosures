{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2412f9c6-2329-40f2-bdc7-6faaef09d137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in c:\\users\\butte\\miniconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from bertopic) (0.8.40)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from bertopic) (0.5.9.post2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from bertopic) (2.3.5)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from bertopic) (2.3.3)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from bertopic) (6.5.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from bertopic) (1.7.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from bertopic) (5.1.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from bertopic) (4.67.1)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from bertopic) (0.45.1)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (2.12.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\butte\\miniconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.57.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\butte\\miniconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\butte\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.20.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\butte\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\butte\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\butte\\miniconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.62.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\butte\\miniconda3\\lib\\site-packages (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: google-genai in c:\\users\\butte\\miniconda3\\lib\\site-packages (1.52.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-genai) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-genai) (2.43.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-genai) (2.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.0)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\butte\\miniconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\butte\\miniconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\butte\\miniconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bertopic\n",
    "%pip install beautifulsoup4\n",
    "%pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d15a22-0c91-4335-91c3-0b2ef3fcf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the file path; this should be changed per file\n",
    "file_path = r\"disclosure.txt\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(file_path)\n",
    "with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    file_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc687392-0ad0-4372-8985-c21b66010860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\bertopic\\_bertopic.py:41\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhdbscan\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HDBSCAN\n\u001b[32m     43\u001b[39m     HAS_HDBSCAN = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\hdbscan\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhdbscan_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HDBSCAN, hdbscan\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrobust_single_linkage_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobustSingleLinkage, robust_single_linkage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\hdbscan\\hdbscan_.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, ClusterMixin\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn.utils'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ENGLISH_STOP_WORDS\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\bertopic\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbertopic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[32m      5\u001b[39m __version__ = version(\u001b[33m\"\u001b[39m\u001b[33mbertopic\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m __all__ = [\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBERTopic\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\bertopic\\_bertopic.py:46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m):\n\u001b[32m     45\u001b[39m     HAS_HDBSCAN = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HDBSCAN \u001b[38;5;28;01mas\u001b[39;00m SK_HDBSCAN\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m sklearn_version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn.utils'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bertopic import BERTopic\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# Step 1: Load file\n",
    "with open(\"disclosure.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Step 2: Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Step 3: Remove unwanted HTML elements\n",
    "for tag in soup([\"script\", \"style\", \"img\", \"footer\", \"nav\"]):\n",
    "    tag.decompose()\n",
    "\n",
    "# Optional: Remove tables entirely (comment out if you want table text preserved)\n",
    "for tag in soup.find_all(\"table\"):\n",
    "    tag.decompose()\n",
    "\n",
    "# Step 4: Extract text with line breaks preserved\n",
    "text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "# Step 5: Clean formatting while keeping newlines\n",
    "# - Replace multiple spaces with one\n",
    "# - Replace three or more newlines with two (keeps paragraph structure)\n",
    "text = re.sub(r\" +\", \" \", text)                  # collapse spaces\n",
    "text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)           # collapse excessive blank lines\n",
    "text = text.strip()\n",
    "\n",
    "# Optional: Remove boilerplate phrases\n",
    "stop_phrases = [\n",
    "    \"table of contents\",\n",
    "    \"copyright\",\n",
    "    \"page\",\n",
    "]\n",
    "\n",
    "for phrase in stop_phrases:\n",
    "    text = re.sub(fr\"(?i){phrase}\", \"\", text)    # case-insensitive replace\n",
    "\n",
    "# Step 6: Save cleaned output (overwrite original)\n",
    "output_path = \"disclosure_cleaned.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(text)\n",
    "\n",
    "print(f\"Cleaned text saved to: {output_path}\")\n",
    "\n",
    "import re\n",
    "\n",
    "# Load cleaned text\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 1: Split into paragraphs (two or more newlines treated as a separator)\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
    "\n",
    "# Step 2: Define what \"big\" means\n",
    "MIN_WORDS = 50  # adjust as needed\n",
    "\n",
    "# Step 3: Filter large paragraphs\n",
    "large_paragraphs = [p.strip() for p in paragraphs if len(p.split()) >= MIN_WORDS]\n",
    "\n",
    "large_paragraphs = [\n",
    "    re.sub(r\"\\d+\", \"\", p).strip()\n",
    "    for p in paragraphs\n",
    "    if len(p.split()) >= MIN_WORDS\n",
    "]\n",
    "\n",
    "### STEP 4 â€” STOPWORD REMOVAL ###\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word.lower() not in ENGLISH_STOP_WORDS])\n",
    "\n",
    "large_paragraphs = [remove_stopwords(p) for p in large_paragraphs]\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "large_paragraphs = [\n",
    "    \" \".join(lemm.lemmatize(word) for word in p.split())\n",
    "    for p in large_paragraphs\n",
    "]\n",
    "\n",
    "\n",
    "### STEP 5 â€” RUN BERTopic (default embeddings) ###\n",
    "print(\"âŒ› Running BERTopic...\")\n",
    "topic_model = BERTopic()  # <â€” no embedding model passed\n",
    "\n",
    "topics, probs = topic_model.fit_transform(large_paragraphs)\n",
    "\n",
    "\n",
    "### STEP 6 â€” PRINT RESULTS ###\n",
    "print(\"\\n==================== TOPIC SUMMARY ====================\")\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)\n",
    "\n",
    "print(\"\\n==================== TOPIC KEYWORDS ====================\")\n",
    "for topic_id in topic_info[\"Topic\"]:\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "    print(f\"\\nðŸ”¹ Topic {topic_id}:\")\n",
    "    for word, score in topic_model.get_topic(topic_id):\n",
    "        print(f\"   {word} ({round(score, 4)})\")\n",
    "\n",
    "print(\"\\n==================== TOPIC EXAMPLES ====================\")\n",
    "for topic_id in set(topics):\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "\n",
    "    example = large_paragraphs[topics.index(topic_id)]\n",
    "    print(f\"\\nðŸŸ¦ Topic {topic_id} Example Paragraph:\\n{example[:600]}...\\n\")\n",
    "\n",
    "print(\"\\nâœ” Topic modeling completed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e078564-ed75-4b35-bbb0-5dc324c479bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prompt is meant to take in a .txt file with the BERTopic of one financial document\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an expert financial analyst. Analyze the following financial document.\n",
    "\n",
    "It is BERTopic analysis of a financial document. The format is as follows:\n",
    "(0, '0.027*\"accounting\" + 0.023*\"audit\" + 0.018*\"auditor\" + ...')\n",
    "(1, '0.021*\"shareholder\" + 0.013*\"proxy\" + ...')\n",
    "\n",
    "- A topic ID at the beginning of the parenthesis parentheses\n",
    "- The weight of a word ascribed to a topic in the article\n",
    "- Followed by the word itself in double quotes\n",
    "\n",
    "You are to calculate the the topic proportion for each topic ID by adding up each word assigned to the topic's weight.\n",
    "Select all topics with an overall weight over .01 and add them to a table in this format:\n",
    "| Topic ID | FileName | Date | Topic Number | Topic Proportion |\n",
    "\n",
    "and then create a second key table with each topic chosen for the first table. decide on topic name by using the words\n",
    "assigned to each topic to guess its overall theme. Format the second table like this:\n",
    "| Topic Number | Topic Name |\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b008da6-7d55-485b-888a-5da2cc23c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "def summarize_disclosure(file_path, prompt):\n",
    "    \"\"\"\n",
    "    Summarizes the content of a large text file using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the text file.\n",
    "        prompt (str): The summarization instruction from the user.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1. Configure the Gemini API Key --- \n",
    "        # !! Security !!: use an environment variable for your API key for security. \n",
    "        # The key can be set as GOOGLE_API_KEY or GEMINI_API_KEY. \n",
    "        # After prototype is reviewed, we will discuss more about security best practices.\n",
    "        api_key = \"AIzaSyDNjiL5E4S5FPBf0naOiGpgfZBd08mIg8M\" # directly insert API key\n",
    "\n",
    "        genai.configure(api_key=api_key)\n",
    "\n",
    "        # Create the model, ! IMPORTANT !: specifying a supported model name\n",
    "        #model = genai.GenerativeModel('gemini-2.5-pro')\n",
    "\n",
    "        # --- 1. Initialize the Generative Model ---\n",
    "        # Can choose different models, should match the one used above.\n",
    "        model = genai.GenerativeModel('gemini-2.5-pro')\n",
    "\n",
    "        # --- 2. Create the Full Prompt for the API ---\n",
    "        # Combining our prompt with the file content.\n",
    "        full_prompt = f\"{prompt}\\n\\n---\\n\\n{file_text}\"\n",
    "\n",
    "        # --- 3. Generate the Summary ---\n",
    "        print(\"Generating summary with Gemini...\")\n",
    "        response = model.generate_content(full_prompt)\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The file '{file_path}' was not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Text File Summarization with Gemini API ---\")\n",
    "\n",
    "    # Get the summary\n",
    "    summary = summarize_disclosure(file_path, prompt)\n",
    "\n",
    "    # Print the result\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
